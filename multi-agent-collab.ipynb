{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "301faede-d393-4031-bac1-3b6e7a3e5b40",
   "metadata": {},
   "source": [
    "# Multi-agent Collaboration\n",
    "\n",
    "A single agent can usually perform well using a small set of tools to solve a specific task. However, even powerful models like GPT-4 may struggle when given many different tools to solve a variety of tasks.\n",
    "\n",
    "One way to approach complicated tasks is through a _\"divide-and-conquer\"_ approach. Create a specialized agent for each task and route tasks to the correct _\"expert\"_.\n",
    "\n",
    "The code from this notebook is adapted from the [LangGraph tutorial](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration). The ideas in this notebook is inspired by the paper [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation](https://arxiv.org/abs/2308.08155), by Wu, et. al.\n",
    "\n",
    "The graph that we'll build will look something like the following diagram:\n",
    "\n",
    "![multi_agent diagram](img/simple_multi_agent_diagram.png)\n",
    "\n",
    "**Figure 1**: Image from [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/multi_agent/multi-agent-collaboration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfa4f28-20b3-4d6c-9f63-feddb69df1eb",
   "metadata": {},
   "source": [
    "## Create Agents\n",
    "\n",
    "The following helper function will help create agents. These agents will then be nodes in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cb86ca1-14d6-49ba-baf8-7d88cd24df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "\n",
    "def create_agent(\n",
    "    model: BaseChatModel, \n",
    "    tools: list[BaseTool], \n",
    "    system_message: str\n",
    ") -> RunnableSequence:\n",
    "    \"\"\"\n",
    "    Create an agent.\n",
    "\n",
    "    Args:\n",
    "        model: The LLM that powers this agent.\n",
    "        tools: List of tools to be called by the LLM.\n",
    "        system_message: The system message passed to the LLM \n",
    "            to configure its behaviour.\n",
    "\n",
    "    Returns:\n",
    "        A chain that can be invoked.\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, \"\n",
    "                \"collaborating with other assistants. \"\n",
    "                \"Use the provided tools to progress \"\n",
    "                \"towards answering the question. \"\n",
    "                \"If you are unable to fully answer, \"\n",
    "                \"that's OK, another assistant with \"\n",
    "                \"different tools will help where \"\n",
    "                \"you left off. Execute what you can \"\n",
    "                \"to make progress. If you or any of \"\n",
    "                \"the other assistants have the \"\n",
    "                \"final answer or deliverable, \"\n",
    "                \"prefix your response with FINAL ANSWER \"\n",
    "                \"so the team knows to stop. \"\n",
    "                \"You have access to the following tools: {tool_names}.\"\n",
    "                \"\\n\\n\"\n",
    "                \"{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(\n",
    "                variable_name=\"messages\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(\n",
    "        system_message=system_message\n",
    "    )\n",
    "    prompt = prompt.partial(\n",
    "        tool_names=\", \".join(\n",
    "            tool.name for tool in tools\n",
    "        )\n",
    "    )\n",
    "    return (\n",
    "        prompt \n",
    "        | model.bind_tools(tools)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7059706d-d8a3-44f5-aeef-d37ad74d60df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import (\n",
    "    START,\n",
    "    END, \n",
    "    StateGraph, \n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
